#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
apply_weight_convert.py
~~~~~~~~~~~~~~~~~~~~
å°† Qwen-2/3ã€Llamaã€LLaVA ç­‰ HuggingFace / PyTorch-bin æƒé‡
æ•´ç†ä¸º lite_llama æ¡†æ¶çš„è‡ªå®šä¹‰æ ¼å¼æ¨¡å‹æƒé‡çš„å°å·¥å…·ã€‚

Usage
-----
python lite_llama/apply_weight_convert.py /path/to/weights [--model-type qwen3] [--device cuda]

Author: harleyszhang (2025-06-08)
"""
from __future__ import annotations

import argparse
import logging
import shutil
from pathlib import Path
from typing import Dict, Mapping

import torch
from tqdm.auto import tqdm
from transformers import (AutoConfig, AutoModelForCausalLM,
                          LlavaConfig, LlavaForConditionalGeneration)

# --------------------------------------------------------------------------- #
# æ—¥å¿—é…ç½®
# --------------------------------------------------------------------------- #
logging.basicConfig(format="%(levelname)s | %(message)s", level=logging.INFO)
logger = logging.getLogger(__name__)

# --------------------------------------------------------------------------- #
# é€šç”¨å·¥å…·å‡½æ•°
# --------------------------------------------------------------------------- #
def ensure_dir(path: Path) -> Path:
    """è‹¥ç›®å½•ä¸å­˜åœ¨åˆ™åˆ›å»ºï¼Œæœ€åè¿”å›è‡ªèº«ã€‚"""
    path.mkdir(parents=True, exist_ok=True)
    return path


def save_state_dict(out_dir: Path, model_id: str, state: Dict[str, torch.Tensor]) -> None:
    """ä¿å­˜ state_dict å¹¶æ‰“å°ä¿¡æ¯ã€‚"""
    torch.save(state, out_dir / f"{model_id}.pth", _use_new_zipfile_serialization=True)
    logger.info("âœ… å·²ä¿å­˜æƒé‡åˆ° %s", out_dir / f"{model_id}.pth")


def copy_metadata(src: Path, dst: Path) -> None:
    """å¤åˆ¶ *.json ä¸ tokenizer.model ç­‰è¾…åŠ©æ–‡ä»¶ã€‚"""
    for file in src.glob("*.json"):
        shutil.copy2(file, dst)
    tok = src / "tokenizer.model"
    if tok.exists():
        shutil.copy2(tok, dst)


# --------------------------------------------------------------------------- #
# ä¿®è®¢åçš„ merge_kv_weights â€”â€” åªç”Ÿæˆ kv_proj_weightï¼Œä¸‹åˆ’çº¿é£æ ¼
# --------------------------------------------------------------------------- #
def merge_kv_weights(state: Dict[str, torch.Tensor],
                     prefix: str,
                     with_bias: bool = False) -> None:
    """
    å°† K/V æŠ•å½±åˆå¹¶ä¸º kv_proj_weight / kv_proj_biasï¼ˆå¯é€‰ï¼‰ï¼Œ
    å®Œå…¨ä½¿ç”¨ **ä¸‹åˆ’çº¿** é”®åï¼Œä¿è¯ä¸ lite-llama çš„ Qwen3 å®ç°ä¸€è‡´ã€‚
    åŒæ—¶è‹¥æ­¤å‰è½¬æ¢è¿‡ç•™ä¸‹æ—§çš„ kv_proj.weightï¼Œä¹Ÿä¼šè¢«åˆ é™¤ã€‚
    """
    # ---------- 1. æ‰¾åˆ°ç°æœ‰ K/V ----------
    # ä¸¤ç§å€™é€‰å‘½åï¼šç‚¹å·é£æ ¼  layers.0.self_attn.k_proj.weight
    #            ä¸‹åˆ’çº¿é£æ ¼ layers.0.self_attn.k_proj_weight
    candidates = [
        (f"{prefix}.k_proj.weight", f"{prefix}.v_proj.weight"),
        (f"{prefix}.k_proj_weight", f"{prefix}.v_proj_weight"),
    ]
    for k_key, v_key in candidates:
        if k_key in state and v_key in state:
            break
    else:  # æ²¡æœ‰ä»»ä½•ä¸€å¯¹åŒ¹é…
        return

    # ---------- 2. åˆå¹¶æƒé‡ ----------
    fused_k = f"{prefix}.kv_proj_weight"            # ç›®æ ‡é”®ï¼ˆä¸‹åˆ’çº¿ï¼‰
    state[fused_k] = torch.cat([state[k_key], state[v_key]], dim=0)
    del state[k_key], state[v_key]

    # ---------- 3. åˆå¹¶ biasï¼ˆå¯é€‰ï¼‰ ----------
    if with_bias:
        bias_cands = [
            (f"{prefix}.k_proj.bias",  f"{prefix}.v_proj.bias"),
            (f"{prefix}.k_proj_bias",  f"{prefix}.v_proj_bias"),
        ]
        for kb_key, vb_key in bias_cands:
            if kb_key in state and vb_key in state:
                fused_b = f"{prefix}.kv_proj_bias"
                state[fused_b] = torch.cat([state[kb_key], state[vb_key]], dim=0)
                del state[kb_key], state[vb_key]
                break

    # ---------- 4. å¦‚æœ‰æ—§ç‰ˆ kv_proj.weightï¼Œé¡ºå¸¦åˆ æ‰ ----------
    old_key = f"{prefix}.kv_proj.weight"
    if old_key in state:
        del state[old_key]


def build_mapping(common: Mapping[str, str],
                  layer_tpl: Mapping[str, str],
                  num_layers: int) -> Dict[str, str]:
    """æ ¹æ®å±‚æ•°å±•å¼€æ¨¡æ¿æ˜ å°„è¡¨ã€‚"""
    mapping = dict(common)
    for i in range(num_layers):
        mapping.update({hf.format(i=i): custom.format(i=i) for hf, custom in layer_tpl.items()})
    return mapping

# --------------------------------------------------------------------------- #
# å…·ä½“å„æ¨¡å‹æ˜ å°„è§„åˆ™
# --------------------------------------------------------------------------- #
_SPEC = {
    # Qwen-2
    "qwen2": {
        "common": {
            "model.norm.weight":         "norm_weight",
            "model.embed_tokens.weight": "embed_tokens.weight",
            "lm_head.weight":            "lm_head_weight",
        },
        "layer": {
            # q_proj/k_proj/... åŒä¸‹
            "model.layers.{i}.self_attn.q_proj.weight":  "layers.{i}.self_attn.q_proj_weight",
            "model.layers.{i}.self_attn.q_proj.bias":    "layers.{i}.self_attn.q_proj_bias",
            "model.layers.{i}.self_attn.k_proj.weight":  "layers.{i}.self_attn.k_proj_weight",
            "model.layers.{i}.self_attn.k_proj.bias":    "layers.{i}.self_attn.k_proj_bias",
            "model.layers.{i}.self_attn.v_proj.weight":  "layers.{i}.self_attn.v_proj_weight",
            "model.layers.{i}.self_attn.v_proj.bias":    "layers.{i}.self_attn.v_proj_bias",
            "model.layers.{i}.self_attn.o_proj.weight":  "layers.{i}.self_attn.o_proj_weight",
            "model.layers.{i}.mlp.gate_proj.weight":     "layers.{i}.mlp.gate_proj.weight",
            "model.layers.{i}.mlp.up_proj.weight":       "layers.{i}.mlp.up_proj.weight",
            "model.layers.{i}.mlp.down_proj.weight":     "layers.{i}.mlp.down_proj.weight",
            "model.layers.{i}.input_layernorm.weight":   "layers.{i}.input_layernorm_weight",
            "model.layers.{i}.post_attention_layernorm.weight": "layers.{i}.post_attention_layernorm_weight",
        },
        "merge_bias": True,
    },

    # Qwen-3
    "qwen3": {
        "common": {
            "model.embed_tokens.weight": "embed_tokens.weight",
            "model.norm.weight":         "norm_weight",
            "lm_head.weight":            "lm_head_weight",
        },
        "layer": {
            "model.layers.{i}.self_attn.q_proj.weight": "layers.{i}.self_attn.q_proj_weight",
            "model.layers.{i}.self_attn.k_proj.weight": "layers.{i}.self_attn.k_proj_weight",
            "model.layers.{i}.self_attn.v_proj.weight": "layers.{i}.self_attn.v_proj_weight",
            "model.layers.{i}.self_attn.q_norm.weight": "layers.{i}.self_attn.q_norm_weight",
            "model.layers.{i}.self_attn.k_norm.weight": "layers.{i}.self_attn.k_norm_weight",
            "model.layers.{i}.self_attn.o_proj.weight": "layers.{i}.self_attn.o_proj_weight",
            "model.layers.{i}.mlp.gate_proj.weight":    "layers.{i}.mlp.gate_proj.weight",
            "model.layers.{i}.mlp.up_proj.weight":      "layers.{i}.mlp.up_proj.weight",
            "model.layers.{i}.mlp.down_proj.weight":    "layers.{i}.mlp.down_proj.weight",
            "model.layers.{i}.input_layernorm.weight":  "layers.{i}.input_layernorm_weight",
            "model.layers.{i}.post_attention_layernorm.weight": "layers.{i}.post_attention_layernorm_weight",
        },
        "merge_bias": False,
    },

    # Llama-HF
    "llama": {
        "common": {
            "model.embed_tokens.weight": "embed_tokens.weight",
            "model.norm.weight":         "norm_weight",
            "lm_head.weight":            "lm_head.weight",
        },
        "layer": {
            "model.layers.{i}.self_attn.q_proj.weight": "layers.{i}.self_attn.q_proj.weight",
            "model.layers.{i}.self_attn.k_proj.weight": "layers.{i}.self_attn.k_proj.weight",
            "model.layers.{i}.self_attn.v_proj.weight": "layers.{i}.self_attn.v_proj.weight",
            "model.layers.{i}.self_attn.o_proj.weight": "layers.{i}.self_attn.o_proj.weight",
            "model.layers.{i}.mlp.gate_proj.weight":    "layers.{i}.mlp.gate_proj.weight",
            "model.layers.{i}.mlp.up_proj.weight":      "layers.{i}.mlp.up_proj.weight",
            "model.layers.{i}.mlp.down_proj.weight":    "layers.{i}.mlp.down_proj.weight",
            "model.layers.{i}.input_layernorm.weight":  "layers.{i}.attention_norm_weight",
            "model.layers.{i}.post_attention_layernorm.weight": "layers.{i}.ffn_norm_weight",
        },
        "merge_bias": False,
    },

    # Llama-binï¼ˆåŸ Fairseq/Llama.PTH æ ¼å¼ï¼‰
    "llama-bin": {
        "common": {
            "tok_embeddings.weight": "embed_tokens.weight",
            "norm.weight":           "norm_weight",
            "output.weight":         "lm_head.weight",
        },
        "layer": {
            "layers.{i}.attention.wq.weight": "layers.{i}.attention.q_proj.weight",
            "layers.{i}.attention.wk.weight": "layers.{i}.attention.k_proj.weight",
            "layers.{i}.attention.wv.weight": "layers.{i}.attention.v_proj.weight",
            "layers.{i}.attention.wo.weight": "layers.{i}.attention.o_proj.weight",
            "layers.{i}.feed_forward.w1.weight": "layers.{i}.feed_forward.gate_proj.weight",
            "layers.{i}.feed_forward.w3.weight": "layers.{i}.feed_forward.up_proj.weight",
            "layers.{i}.feed_forward.w2.weight": "layers.{i}.feed_forward.down_proj.weight",
            "layers.{i}.attention_norm.weight":  "layers.{i}.attention_norm_weight",
            "layers.{i}.ffn_norm.weight":        "layers.{i}.ffn_norm_weight",
        },
        "merge_bias": False,
    },

    # LLaVA-Llama
    "llava": {
        "common": {
            "language_model.model.embed_tokens.weight": "language_model.embed_tokens.weight",
            "language_model.model.norm.weight":         "language_model.norm_weight",
            "language_model.lm_head.weight":            "language_model.lm_head.weight",
        },
        "layer": {
            "language_model.model.layers.{i}.self_attn.q_proj.weight": "language_model.layers.{i}.self_attn.q_proj.weight",
            "language_model.model.layers.{i}.self_attn.k_proj.weight": "language_model.layers.{i}.self_attn.k_proj.weight",
            "language_model.model.layers.{i}.self_attn.v_proj.weight": "language_model.layers.{i}.self_attn.v_proj.weight",
            "language_model.model.layers.{i}.self_attn.o_proj.weight": "language_model.layers.{i}.self_attn.o_proj.weight",
            "language_model.model.layers.{i}.mlp.gate_proj.weight":    "language_model.layers.{i}.mlp.gate_proj.weight",
            "language_model.model.layers.{i}.mlp.up_proj.weight":      "language_model.layers.{i}.mlp.up_proj.weight",
            "language_model.model.layers.{i}.mlp.down_proj.weight":    "language_model.layers.{i}.mlp.down_proj.weight",
            "language_model.model.layers.{i}.input_layernorm.weight":  "language_model.layers.{i}.attention_norm_weight",
            "language_model.model.layers.{i}.post_attention_layernorm.weight": "language_model.layers.{i}.ffn_norm_weight",
        },
        "merge_bias": False,
    },
}

# --------------------------------------------------------------------------- #
# æ ¸å¿ƒè½¬æ¢é€»è¾‘
# --------------------------------------------------------------------------- #
def convert(checkpoints_dir: Path,
            hf_state: Dict[str, torch.Tensor],
            model_type: str,
            num_layers: int) -> Dict[str, torch.Tensor]:
    """æ‰§è¡Œä¸»è½¬æ¢æµç¨‹å¹¶æŠŠç»“æœä¿å­˜åˆ° my_weight/<model_id>/ ç›®å½•ã€‚"""
    spec = _SPEC[model_type]
    mapping = build_mapping(spec["common"], spec["layer"], num_layers)
    new_sd: Dict[str, torch.Tensor] = {}

    # ---------- 1. é‡æ˜ å°„ ----------
    for k, v in tqdm(hf_state.items(), desc=f"[{model_type}] æƒé‡é‡æ˜ å°„"):
        if (ck := mapping.get(k)) is not None:
            new_sd[ck] = v
        else:
            logger.debug("å¿½ç•¥æœªæ˜ å°„å‚æ•° %s", k)

    # ---------- 2. ä»…å¯¹ *Qwen* ç³»åˆ—æ‰§è¡Œ KV åˆå¹¶ ----------
    if model_type.startswith("qwen") or model_type.startswith("llama"):              # åªå¤„ç† Qwen-2 / Qwen-3 ç­‰
        for i in range(num_layers):
            prefix = f"layers.{i}.self_attn"       # Qwen æ— é¢å¤–å‰ç¼€
            merge_kv_weights(new_sd, prefix, with_bias=spec["merge_bias"])

    # ---------- 3. ä¿å­˜ ----------
    script_root = Path(__file__).resolve().parent
    out_dir = ensure_dir(script_root / "my_weight" / checkpoints_dir.name)
    save_state_dict(out_dir, checkpoints_dir.name, new_sd)
    copy_metadata(checkpoints_dir, out_dir)

    logger.info("ğŸ‰ è½¬æ¢å®Œæˆï¼Œå…± %d ä¸ªå‚æ•°", len(new_sd))
    return new_sd



# --------------------------------------------------------------------------- #
# CLI è¾…åŠ©ï¼šç”± config.json åˆ¤åˆ«æ¨¡å‹ç±»å‹
# --------------------------------------------------------------------------- #
def detect_model_type(checkpoints_dir: Path) -> str:
    """
    è¯»å– config.json ä¸­çš„ model_type å­—æ®µã€‚
    è‹¥è¯¥å­—æ®µåœ¨ _SPEC ä¸­æ— æ³•æ‰¾åˆ°ï¼Œåˆ™æŠ›å‡ºé”™è¯¯æç¤ºã€‚
    """
    cfg = AutoConfig.from_pretrained(checkpoints_dir, trust_remote_code=True)
    mtype = cfg.model_type.lower()
    # æŸäº›æ¨¡å‹å¯èƒ½éœ€è¦é¢å¤–å½’ä¸€åŒ– / æ˜ å°„
    alias = {
        "qwen2":   "qwen2",
        "qwen3":   "qwen3",
        "llama":   "llama",
        "llava":   "llava",
    }.get(mtype, mtype)      # é»˜è®¤åŸæ ·è¿”å›
    if alias not in _SPEC:
        raise ValueError(f"æš‚ä¸æ”¯æŒçš„ model_type '{mtype}'ï¼Œè¯·æ£€æŸ¥æ˜ å°„è¡¨")
    return alias


def load_hf_state(checkpoints_dir: Path,
                  model_type: str,
                  device: str = "cpu") -> Dict[str, torch.Tensor]:
    """åŠ è½½ HF / bin æƒé‡åˆ° state_dictã€‚"""
    if model_type == "llava":
        model = (LlavaForConditionalGeneration
                 .from_pretrained(checkpoints_dir, torch_dtype=torch.float16, low_cpu_mem_usage=True)
                 .to(device))
    else:
        model = (AutoModelForCausalLM
                 .from_pretrained(checkpoints_dir, torch_dtype=torch.float16, low_cpu_mem_usage=True)
                 .to(device))
    return model.state_dict()


def get_num_layers(checkpoints_dir: Path, model_type: str) -> int:
    """ä» config ä¸­æå– Transformer å±‚æ•°ã€‚"""
    if model_type == "llava":
        cfg = LlavaConfig.from_pretrained(checkpoints_dir)
        return cfg.text_config.num_hidden_layers
    cfg = AutoConfig.from_pretrained(checkpoints_dir)
    return cfg.num_hidden_layers


def main() -> None:
    parser = argparse.ArgumentParser(
        description="Convert HF / bin checkpoints into Lite-LLaMA format.")
    parser.add_argument("checkpoints_dir", type=Path, help="æ¨¡å‹æƒé‡ç›®å½•")
    parser.add_argument("--model-type",
                        choices=_SPEC.keys(),
                        help="æ˜¾å¼æŒ‡å®šæ¨¡å‹ç±»å‹ï¼›é»˜è®¤æ ¹æ®ç›®å½•åçŒœæµ‹")
    parser.add_argument("--device", default="cuda",
                        help="åŠ è½½æƒé‡æ—¶ä½¿ç”¨çš„è®¾å¤‡ (default: cuda)")
    args = parser.parse_args()

    ckpt_dir: Path = args.checkpoints_dir.resolve()
    
    # 1ï¸âƒ£ **ç›´æ¥ä» config.json è¯»å– model_type** â†“
    model_type = detect_model_type(ckpt_dir)
    logger.info("æ£€æµ‹åˆ° model_type = %s", model_type)

    # 2ï¸âƒ£ è·å–å±‚æ•°
    num_layers = get_num_layers(ckpt_dir, model_type)
    logger.info("Transformer å±‚æ•° %d", num_layers)

    # 3ï¸âƒ£ åŠ è½½æƒé‡å¹¶æ‰§è¡Œè½¬æ¢
    hf_sd = load_hf_state(ckpt_dir, model_type, device=args.device)
    convert(ckpt_dir, hf_sd, model_type, num_layers)

# tests/test_convert.py
import torch
import json
from pathlib import Path
from types import SimpleNamespace
import pytest

# ---------- å…¬å…±å·¥å…· ----------
def dummy_state_dict_qwen(num_layers: int, hidden: int = 4):
    """æ„é€ ä¸€ä¸ªæœ€å°åŒ–çš„ Qwen state_dictï¼Œä»…åŒ…å« KV/O/W1/W2 ... å±‚æƒé‡ã€‚"""
    sd = {
        "model.norm.weight": torch.ones(hidden),
        "model.embed_tokens.weight": torch.zeros(10, hidden),
        "lm_head.weight": torch.zeros(hidden, 10),
    }
    for i in range(num_layers):
        prefix = f"model.layers.{i}"
        sd.update({
            f"{prefix}.self_attn.q_proj.weight": torch.randn(hidden, hidden),
            f"{prefix}.self_attn.k_proj.weight": torch.randn(hidden, hidden),
            f"{prefix}.self_attn.v_proj.weight": torch.randn(hidden, hidden),
            f"{prefix}.self_attn.o_proj.weight": torch.randn(hidden, hidden),
            f"{prefix}.mlp.gate_proj.weight": torch.randn(2 * hidden, hidden),
            f"{prefix}.mlp.up_proj.weight":   torch.randn(2 * hidden, hidden),
            f"{prefix}.mlp.down_proj.weight": torch.randn(hidden, 2 * hidden),
            f"{prefix}.input_layernorm.weight": torch.ones(hidden),
            f"{prefix}.post_attention_layernorm.weight": torch.ones(hidden),
        })
    return sd


def dummy_state_dict_llama(num_layers: int, hidden: int = 4):
    """æ„é€ æœ€å°åŒ– Llama HF æƒé‡ï¼Œæ–¹ä¾¿æµ‹è¯•ä¸è¿›è¡Œ KV åˆå¹¶ã€‚"""
    sd = {
        "model.norm.weight": torch.ones(hidden),
        "model.embed_tokens.weight": torch.zeros(10, hidden),
        "lm_head.weight": torch.zeros(hidden, 10),
    }
    for i in range(num_layers):
        prefix = f"model.layers.{i}"
        sd.update({
            f"{prefix}.self_attn.q_proj.weight": torch.randn(hidden, hidden),
            f"{prefix}.self_attn.k_proj.weight": torch.randn(hidden, hidden),
            f"{prefix}.self_attn.v_proj.weight": torch.randn(hidden, hidden),
            f"{prefix}.self_attn.o_proj.weight": torch.randn(hidden, hidden),
            f"{prefix}.mlp.gate_proj.weight": torch.randn(2 * hidden, hidden),
            f"{prefix}.mlp.up_proj.weight":   torch.randn(2 * hidden, hidden),
            f"{prefix}.mlp.down_proj.weight": torch.randn(hidden, 2 * hidden),
            f"{prefix}.input_layernorm.weight": torch.ones(hidden),
            f"{prefix}.post_attention_layernorm.weight": torch.ones(hidden),
        })
    return sd


# ---------- fixtures ----------
@pytest.fixture(scope="function")
def tmp_ckpt_dir(tmp_path: Path):
    """åˆ›å»ºä¸´æ—¶ checkpoints ç›®å½•å¹¶å†™å…¥æœ€å° config.jsonã€‚"""
    def _factory(model_type: str):
        ckpt = tmp_path / model_type
        ckpt.mkdir()
        (ckpt / "config.json").write_text(json.dumps({"model_type": model_type}))
        return ckpt
    return _factory


# ---------- æµ‹è¯•æ˜ å°„ ----------
@pytest.mark.parametrize("model_type", ["qwen3", "llama"])
def test_mapping_and_num_params(tmp_ckpt_dir, model_type):
    ckpt_dir = tmp_ckpt_dir(model_type)
    num_layers = 2

    # æ„é€ å‡æƒé‡
    sd = (dummy_state_dict_qwen(num_layers) if model_type.startswith("qwen")
          else dummy_state_dict_llama(num_layers))

    # è·‘è½¬æ¢
    new_sd = convert(ckpt_dir, sd, model_type, num_layers)

    # åŸºç¡€ key åº”å½“å­˜åœ¨
    assert "embed_tokens.weight" in new_sd
    assert "norm_weight" in new_sd


# ---------- Qwen KV åˆå¹¶ ----------
def test_qwen_kv_merge(tmp_ckpt_dir):
    model_type = "qwen3"
    num_layers = 1
    ckpt_dir = tmp_ckpt_dir(model_type)
    sd = dummy_state_dict_qwen(num_layers)

    new_sd = convert(ckpt_dir, sd, model_type, num_layers)

    # KV fused weight åº”å‡ºç°
    kv_key = "layers.0.self_attn.kv_proj.weight"
    assert kv_key in new_sd

    # åŸ Kã€V ä¸åº”ä¿ç•™
    assert "layers.0.self_attn.k_proj_weight" not in new_sd
    assert "layers.0.self_attn.v_proj_weight" not in new_sd

    # ç»´åº¦æ£€æŸ¥ï¼šconcat åç¬¬ä¸€ç»´åº”ä¸º 2*hidden
    hidden = sd["model.norm.weight"].numel()     # 4
    assert new_sd[kv_key].shape[0] == 2 * hidden


# ---------- Llama ä¸åˆå¹¶ ----------
def test_llama_no_kv_merge(tmp_ckpt_dir):
    model_type = "llama"
    num_layers = 1
    ckpt_dir = tmp_ckpt_dir(model_type)
    sd = dummy_state_dict_llama(num_layers)

    new_sd = convert(ckpt_dir, sd, model_type, num_layers)

    # KV fused ä¸å­˜åœ¨
    assert "layers.0.self_attn.kv_proj.weight" not in new_sd
    # åŸ K/V ä»ç„¶å­˜åœ¨
    assert "layers.0.self_attn.k_proj.weight" in new_sd
    assert "layers.0.self_attn.v_proj.weight" in new_sd

if __name__ == "__main__":
    main()
